{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student first name:** *Aman* \n",
    "\n",
    "**Student last name:** *Kawatra* \n",
    "\n",
    "**Student ANU UNI ID:** *u6070546*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMP4650 Social Media Assignment Submission (10 points)\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Assignment questions related to tutorial 1: Constructing and analyzing a social network\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This initial code is necessary for loading into this notebook's Python kernel the object necessary for this part of the assignment. Run the next cell before starting to work on the assignment questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "import csv\n",
    "import urllib2\n",
    "import networkx as nx\n",
    "\n",
    "# opening a local CSV file\n",
    "to_read = file(\"./dataset.csv\")  #use this line for a locally downloaded file\n",
    "# or reading it directly from the specified URL\n",
    "# url = 'http://rizoiu.eu/sna-lab-ipython/dataset.csv'\n",
    "# to_read = urllib2.urlopen(url)\n",
    "reader = csv.reader(to_read)\n",
    "\n",
    "#  construct the networkx graph\n",
    "G = nx.Graph()\n",
    "for line in reader:\n",
    "    if line[0] not in G:  G.add_node(line[0])\n",
    "    if line[1] not in G:  G.add_node(line[1])\n",
    "    G.add_edge(line[0], line[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 point) Assignment question #1.1:** Find another way of determining the number of users, by applying a (networkx) graph method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The social graph contains 217 users.\n",
      "The social graph contains 217 users.\n",
      "The social graph contains 296 relations.\n"
     ]
    }
   ],
   "source": [
    "# code or answer for question. In case of plain text answer, change the type of cell to \"Raw Text\" using the toolbar.\n",
    "print \"The social graph contains %s users.\" % len(G)\n",
    "\n",
    "# Another method\n",
    "print \"The social graph contains %s users.\" % nx.number_of_nodes(G)\n",
    "\n",
    "print \"The social graph contains %s relations.\" % G.number_of_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points) Assignment question #1.2:** What is the minimum number of introductions required for the user `'137056623'` to reach any other user? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 10 introductions, '137056623' can reach anyone in the network\n"
     ]
    }
   ],
   "source": [
    "# code or answer for question. In case of plain text answer, change the type of cell to \"Raw Text\" using the toolbar.\n",
    "no_hops = max([len(nx.shortest_path(G, source = '137056623', target = x)) - 1 for x in G.nodes()])\n",
    "print \"Using %d introductions, '137056623' can reach anyone in the network\" % no_hops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points) Assignment question #1.3:** What is the minimum number of introductions required for the any user to reach any other user in the  network?  \n",
    "\n",
    "**HINT:** study the `shortest_path_length` method description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 18 introductions, anyone can reach anyone in the network\n"
     ]
    }
   ],
   "source": [
    "# code or answer for question. In case of plain text answer, change the type of cell to \"Raw Text\" using the toolbar.\n",
    "# print nx.shortest_path_length(G)\n",
    "\n",
    "No_hops = 0\n",
    "for y in G:\n",
    "    l = max([len(nx.shortest_path(G, source = y, target = x)) - 1 for x in G.nodes()])\n",
    "    if no_hops < l:\n",
    "        no_hops = l\n",
    "\n",
    "print \"Using %d introductions, anyone can reach anyone in the network\" % no_hops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points) Assignment question #1.4:** The _diameter_ $d$ of a graph is the maximum eccentricity of any node, $d = \\max_{v} \\epsilon(v)$. Give two ways to compute the diameter of the social network G (one using the calculated eccentricity values calculated earlier and another one using the dedicated `networkx` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most central user is '109670729'\n",
      "Radius : 9\n",
      "Diameter : 18\n",
      "Diameter : 18\n"
     ]
    }
   ],
   "source": [
    "# code or answer for question. In case of plain text answer, change the type of cell to \"Raw Text\" using the toolbar.\n",
    "import operator\n",
    "\n",
    "ec = nx.eccentricity(G)\n",
    "print \"The most central user is '%s'\" % min(ec.iteritems(), key=operator.itemgetter(1))[0]\n",
    "\n",
    "print \"Radius : %s\" % min(ec.values())\n",
    "\n",
    "#First Way\n",
    "print \"Diameter : %s\" % nx.diameter(G)\n",
    "\n",
    "# Second way\n",
    "print \"Diameter : %s\" % max(ec.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points) Assignment question #1.5:** Determine ALL most central node(s) and ALL most central edge(s), with respect to the node and edge betweenes centrality. Remember that two or more nodes/edges may have the same centrality score and they ALL need to be determined for this assignment.  \n",
    "**Hint:** `networkx` already contains dedicated function to compute node and edge betweenness centrality scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most central node is 54837666 with centrality 0.648037822069\n",
      "The most central edge is ('2260701138', '54837666') with centrality 0.496671786994\n"
     ]
    }
   ],
   "source": [
    "# code or answer for question. In case of plain text answer, change the type of cell to \"Raw Text\" using the toolbar.\n",
    "cent = nx.betweenness_centrality(G)\n",
    "\n",
    "#for key,val in cent.items():\n",
    "#    print key, \"=>\", val\n",
    "\n",
    "max_val = max(cent.iteritems(), key=operator.itemgetter(1))[0]\n",
    "print \"The most central node is %s with centrality %s\" % (max_val, cent[max_val])\n",
    "\n",
    "c = nx.edge_betweenness_centrality(G)\n",
    "max_v = max(c.iteritems(), key=operator.itemgetter(1))[0]\n",
    "print \"The most central edge is %s with centrality %s\" % (max_v, c[max_v])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Assignment questions related to the tutorial 2: constructing a network from real data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 point) Assignment question #2.1:** Starting from [the same bzipped JSON twitter dataset](https://wattlecourses.anu.edu.au/mod/resource/view.php?id=1215757), construct the social graph based on the reply relation. Analyse (as shown in *Step 3*) which fields you require and give the Python code necessary for constructing the network.  \n",
    "**NOTE:** all the subsequent questions of this subsection are to be solved on the graph constructed at this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"YoutubeID\": \"iS1g8G_njx8\",\n",
      "    \"_id\": {\n",
      "        \"$oid\": \"5398013792ccfc23ee6f74c7\"\n",
      "    },\n",
      "    \"contributors\": null,\n",
      "    \"coordinates\": null,\n",
      "    \"created_at\": {\n",
      "        \"$date\": \"2014-05-30T22:55:45.000+0200\"\n",
      "    },\n",
      "    \"entities\": {\n",
      "        \"hashtags\": [\n",
      "            {\n",
      "                \"indices\": [\n",
      "                    70,\n",
      "                    89\n",
      "                ],\n",
      "                \"text\": \"WatchProblemOnVEVO\"\n",
      "            },\n",
      "            {\n",
      "                \"indices\": [\n",
      "                    90,\n",
      "                    103\n",
      "                ],\n",
      "                \"text\": \"problemvideo\"\n",
      "            }\n",
      "        ],\n",
      "        \"symbols\": [],\n",
      "        \"urls\": [\n",
      "            {\n",
      "                \"display_url\": \"youtu.be/iS1g8G_njx8\",\n",
      "                \"expanded_url\": \"http://youtu.be/iS1g8G_njx8\",\n",
      "                \"indices\": [\n",
      "                    42,\n",
      "                    64\n",
      "                ],\n",
      "                \"url\": \"http://t.co/ji6ETHIFuf\"\n",
      "            }\n",
      "        ],\n",
      "        \"user_mentions\": [\n",
      "            {\n",
      "                \"id\": 34507480,\n",
      "                \"id_str\": \"34507480\",\n",
      "                \"indices\": [\n",
      "                    3,\n",
      "                    16\n",
      "                ],\n",
      "                \"name\": \"Ariana Grande\",\n",
      "                \"screen_name\": \"ArianaGrande\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"favorite_count\": 0,\n",
      "    \"geo\": null,\n",
      "    \"id\": {\n",
      "        \"$numberLong\": \"472481511628111872\"\n",
      "    },\n",
      "    \"in_reply_to_screen_name\": null,\n",
      "    \"in_reply_to_status_id\": null,\n",
      "    \"in_reply_to_status_id_str\": null,\n",
      "    \"in_reply_to_user_id\": null,\n",
      "    \"in_reply_to_user_id_str\": null,\n",
      "    \"lang\": \"en\",\n",
      "    \"place\": null,\n",
      "    \"retweet_count\": 0,\n",
      "    \"retweeted_status\": {\n",
      "        \"contributors\": null,\n",
      "        \"coordinates\": null,\n",
      "        \"created_at\": {\n",
      "            \"$date\": \"2014-05-30T22:05:05.000+0200\"\n",
      "        },\n",
      "        \"entities\": {\n",
      "            \"hashtags\": [\n",
      "                {\n",
      "                    \"indices\": [\n",
      "                        52,\n",
      "                        71\n",
      "                    ],\n",
      "                    \"text\": \"WatchProblemOnVEVO\"\n",
      "                },\n",
      "                {\n",
      "                    \"indices\": [\n",
      "                        72,\n",
      "                        85\n",
      "                    ],\n",
      "                    \"text\": \"problemvideo\"\n",
      "                }\n",
      "            ],\n",
      "            \"symbols\": [],\n",
      "            \"urls\": [\n",
      "                {\n",
      "                    \"display_url\": \"youtu.be/iS1g8G_njx8\",\n",
      "                    \"expanded_url\": \"http://youtu.be/iS1g8G_njx8\",\n",
      "                    \"indices\": [\n",
      "                        24,\n",
      "                        46\n",
      "                    ],\n",
      "                    \"url\": \"http://t.co/ji6ETHIFuf\"\n",
      "                }\n",
      "            ],\n",
      "            \"user_mentions\": []\n",
      "        },\n",
      "        \"favorite_count\": 22358,\n",
      "        \"geo\": null,\n",
      "        \"id\": {\n",
      "            \"$numberLong\": \"472468761396932608\"\n",
      "        },\n",
      "        \"in_reply_to_screen_name\": null,\n",
      "        \"in_reply_to_status_id\": null,\n",
      "        \"in_reply_to_status_id_str\": null,\n",
      "        \"in_reply_to_user_id\": null,\n",
      "        \"in_reply_to_user_id_str\": null,\n",
      "        \"lang\": \"en\",\n",
      "        \"place\": null,\n",
      "        \"retweet_count\": 22872,\n",
      "        \"source\": \"<a href=\\\"http://twitter.com/download/iphone\\\" rel=\\\"nofollow\\\">Twitter for iPhone</a>\",\n",
      "        \"text\": \"here it is y'all ...... http://t.co/ji6ETHIFuf #WatchProblemOnVEVO #problemvideo\",\n",
      "        \"user\": {\n",
      "            \"created_at\": {\n",
      "                \"$date\": \"2009-04-23T04:56:31.000+0200\"\n",
      "            },\n",
      "            \"description\": \"working on a new album for my loves\",\n",
      "            \"favourites_count\": 9522,\n",
      "            \"followers_count\": 15483654,\n",
      "            \"friends_count\": 67815,\n",
      "            \"geo_enabled\": false,\n",
      "            \"id\": 34507480,\n",
      "            \"lang\": \"en\",\n",
      "            \"listed_count\": 39099,\n",
      "            \"location\": \"Honeymoon ave. \",\n",
      "            \"name\": \"Ariana Grande\",\n",
      "            \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/34507480/1399852554\",\n",
      "            \"protected\": false,\n",
      "            \"screen_name\": \"ArianaGrande\",\n",
      "            \"statuses_count\": 33736,\n",
      "            \"time_zone\": \"Pacific Time (US & Canada)\",\n",
      "            \"url\": \"http://smarturl.it/ArianaProblemiT\",\n",
      "            \"utc_offset\": -25200,\n",
      "            \"verified\": true\n",
      "        }\n",
      "    },\n",
      "    \"source\": \"<a href=\\\"http://twitter.com/download/iphone\\\" rel=\\\"nofollow\\\">Twitter for iPhone</a>\",\n",
      "    \"text\": \"RT @ArianaGrande: here it is y'all ...... http://t.co/ji6ETHIFuf #WatchProblemOnVEVO #problemvideo\",\n",
      "    \"user\": {\n",
      "        \"created_at\": {\n",
      "            \"$date\": \"2014-05-21T03:39:06.000+0200\"\n",
      "        },\n",
      "        \"description\": \"Rip Nanny\\u266111.11.13\\u2661\",\n",
      "        \"favourites_count\": 32,\n",
      "        \"followers_count\": 38,\n",
      "        \"friends_count\": 61,\n",
      "        \"geo_enabled\": false,\n",
      "        \"id\": {\n",
      "            \"$numberLong\": \"2556502655\"\n",
      "        },\n",
      "        \"lang\": \"en\",\n",
      "        \"listed_count\": 0,\n",
      "        \"location\": \"\",\n",
      "        \"name\": \"Ally\",\n",
      "        \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/2556502655/1401065330\",\n",
      "        \"protected\": false,\n",
      "        \"screen_name\": \"apetrakiann\",\n",
      "        \"statuses_count\": 51,\n",
      "        \"time_zone\": null,\n",
      "        \"url\": null,\n",
      "        \"utc_offset\": null,\n",
      "        \"verified\": false\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import bz2, json\n",
    "\n",
    "reader = bz2.BZ2File(\"./twitter-dump.json.bz2\", mode=\"r\")\n",
    "jobj = json.loads(reader.readline())\n",
    "\n",
    "print json.dumps(jobj, sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"# cell for analysis of required fields\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# open the bzipped dataset\n",
    "reader = bz2.BZ2File(\"./twitter-dump.json.bz2\", mode=\"r\")\n",
    "#  construct the networkx graph\n",
    "DG = nx.DiGraph()\n",
    "\n",
    "# go line by line (ergo, tweet by tweet)\n",
    "for line in reader:\n",
    "    # load the JSON object from the read line\n",
    "    jobj = json.loads(line)\n",
    "    \n",
    "    # if the current tweet is a retweet\n",
    "    if jobj[u'in_reply_to_user_id'] is not None:\n",
    "        # determine user id\n",
    "        user_id = jobj[u'user'][u'id']\n",
    "        if isinstance(user_id, dict):\n",
    "            user_id = user_id.values()[0]\n",
    "            user_id = int(user_id)\n",
    "    \n",
    "        # determine the retweeted user id\n",
    "        reply_user_id = jobj[u'in_reply_to_user_id']\n",
    "        if isinstance(reply_user_id, dict):\n",
    "            reply_user_id = reply_user_id.values()[0]\n",
    "        reply_user_id = int(reply_user_id)\n",
    "        \n",
    "        # add the two nodes in the graph, if not already there\n",
    "        if user_id not in DG:  DG.add_node(user_id)\n",
    "        if reply_user_id not in DG:  DG.add_node(reply_user_id)\n",
    "        DG.add_edge(user_id, reply_user_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 point) Assignment question #2.2:** How many nodes and edges do you have in the resulted reply graph?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 735\n",
      "Number of edges: 622\n"
     ]
    }
   ],
   "source": [
    "# code or answer for question. In case of plain text answer, change the type of cell to \"Raw Text\" using the toolbar.\n",
    "print \"Number of nodes: %d\" % DG.number_of_nodes()\n",
    "print \"Number of edges: %d\" % DG.number_of_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 point) Assignment question #2.3:** Using the `networkx` functions, filter the reply graph constructed at question #2.1 to its *giant connected component*. \n",
    "Calculate for this new graph: i) the number of nodes, ii) the number of edges, iii) the radius and iv) the diameter.  \n",
    "**HINT:** You can either construct a new graph which contains only the nodes and edges in the *giant connected component* or you can remove from your initial graph all the nodes and edges belonging to the other connected components.  \n",
    "**NOTE:** This question should be solved on the UNDIRECTED version of the social graph. Considering that *DG* is the constructed directed networkx graph, do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 465\n",
      "Radius : 3\n",
      "Diameter : 6\n",
      "The graph is connected, it has only one connected component\n"
     ]
    }
   ],
   "source": [
    "# create an undirected copy of our graph\n",
    "G = DG.to_undirected()\n",
    "G = max(nx.connected_component_subgraphs(G), key=len)\n",
    "ec = nx.eccentricity(G)\n",
    "\n",
    "# code or answer for question. In case of plain text answer, change the type of cell to \"Raw Text\" using the toolbar.\n",
    "print \"Number of nodes: %d\" % len(G)\n",
    "print \"Radius : %s\" % min(ec.values())\n",
    "print \"Diameter : %s\" % nx.diameter(G)\n",
    "\n",
    "if nx.is_connected(G):\n",
    "    print \"The graph is connected, it has only one connected component\"\n",
    "else:\n",
    "    print \"The graph contains %d connected components.\" % nx.number_connected_components(G)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 point) Assignment question #2.4:** This assignment question is to calculate iterativelly the basic form of the PageRank on the reply social graph constructed previously. Considering that *DG* is the constructed directed networkx graph constructed at the **Assignment questions #2.1**. We will construct the *PageRank* on the largest [*weakly connected component*](http://en.wikipedia.org/wiki/Connectivity_%28graph_theory%29). This is constructed as shown hereafter.\n",
    "Your job is to compute the PageRank, using the algorithm provided in the tutorial and exemplified in the lecture notes.\n",
    "Use a maximum of $maxiter = 20$ iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(34507480, 2.463876718330115e-07),\n",
       " (34359954, 6.94190539567217e-09),\n",
       " (153694176, 4.266824215240432e-09),\n",
       " (271239188, 4.262980138522665e-09),\n",
       " (1188471348, 2.555762228846142e-09),\n",
       " (2502937470, 2.290630613621684e-09),\n",
       " (1456140289, 1.6888235912127742e-09),\n",
       " (217733121, 1.6888235912127742e-09),\n",
       " (1059729420, 1.6888235912127742e-09),\n",
       " (307812367, 1.6888235912127742e-09)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "from itertools import islice\n",
    "\n",
    "# get the list of weakly connected components, sorted by size\n",
    "omp_List = sorted(nx.weakly_connected_component_subgraphs(DG), key = len, reverse=True)\n",
    "\n",
    "# we want the first component, the biggest\n",
    "WCDG = omp_List[0]\n",
    "\n",
    "# get the number of nodes of the graph\n",
    "n = len(WCDG.nodes())\n",
    "\n",
    "# initialize the PR at moment 0 using a dictionary of pairs {node : score}\n",
    "PR = {}\n",
    "for node in WCDG.nodes():\n",
    "    PR[node] = 1.0/n\n",
    "\n",
    "# initialize variables\n",
    "no_iter=0     # current iteration\n",
    "iter_max=20  # maximum number of iterations\n",
    "d = 0.7       # decay factor\n",
    "\n",
    "\n",
    "while no_iter < iter_max: # iterate until maximum iterations\n",
    "    \n",
    "    # at each iteration, we compute a new version of the PageRank for each node\n",
    "    new_PR = {}\n",
    "    no_iter = no_iter + 1\n",
    "    \n",
    "    # calculate the PageRank of each node (`new_PR[node]`), based on the previous values (`PR[other_nodes]`)\n",
    "    for node in WCDG.nodes():\n",
    "        ## code for calculating new_PR[node].\n",
    "        neighbors=WCDG.predecessors(node)\n",
    "        value=0.0\n",
    "        for n in neighbors:\n",
    "            value+=PR[n]/len(WCDG.out_edges(n))\n",
    "        new_PR[node]=((1-d)/n)+value\n",
    "    # at the end of iteration, replace old values of PR with the new ones\n",
    "    PR = new_PR\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "# even if a dictionary is not sorted (it is just a collection) we sort it so that the next\n",
    "# printing at the screen shows some interesting values\n",
    "sorted_PR = sorted(PR.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "# and print it out the first 10 elements\n",
    "list(islice(sorted_PR, 10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Assignment questions related to the tutorial 3: sentiment analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is necessary to define the sentiment scoring function that was constructed in the tutorial. Run this before continuing with the assignments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_tweets = [ 'I love this cars',\n",
    "               'This view is amazing',\n",
    "\t           'I feel great this morning',\n",
    "\t           'I am so excited about the concert',\n",
    "\t           'He is my best friend']\n",
    "\n",
    "neg_tweets = ['I do not like this car',\n",
    "              'This view is horrible',\n",
    "              'I feel tired this morning',\n",
    "              'I am not looking forward to the concert',\n",
    "              'He is my worst enemy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# define the lemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "# read the positive and negative lexicon in lists of words\n",
    "positive_words = [lmtzr.lemmatize(line.strip().decode('utf-8')) for line in open('sentiment-lexicon-positive-words.txt')]\n",
    "negative_words = [lmtzr.lemmatize(line.strip().decode('utf-8')) for line in open('sentiment-lexicon-negative-words.txt')]\n",
    "\n",
    "# define the function that computes the sentiment score\n",
    "def get_sentiment_score(text):\n",
    "    # tokenize and lemmatize the current tweet\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tweet = [lmtzr.lemmatize(x.lower()) for x in tokens if len(x) >= 3]\n",
    "\n",
    "    # calculate the sentiment score\n",
    "    score = 0\n",
    "    for word in tweet:\n",
    "        if word in positive_words:\n",
    "            score = score + 1\n",
    "        if word in negative_words:\n",
    "            score = score - 1\n",
    "    return score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 point) Assignment question #3.1:** Use the `get_sentiment_score(text)`, the sentiment scoring function defined before, and calculate the sentiment polarity of the tweets in the Twitter JSON dataset used in [tutorial 2](https://wattlecourses.anu.edu.au/pluginfile.php/1510225/mod_resource/content/14/tutorial-2-construct-network-real-twitter-dump.html). \n",
    "The dataset is [available to download here](https://wattlecourses.anu.edu.au/mod/resource/view.php?id=1215757). \n",
    "Print the text of the 10 most positive and the 10 most negative tweets. \n",
    "We consider that a tweet $t_1$ is more positive than another tweet $t_2$ when score of the former is higher than the score of the latter ($score(t_1) > score(t_2)$). \n",
    "Similarly, a tweet $t_1$ is more negative than $t_2$ when $score(t_1) < score(t_2)$.  \n",
    "**HINT:** Load the tweets one by one as seen in tutorial 2 and extract the text, which is found in the field *text* of each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 tweets:\n",
      "(u'It looks so retro I love it https://t.co/0vg3QPjvvs', 1)\n",
      "(u'RT @juneconey: AAAAAAAAAAAAAAAAAAAAAA https://t.co/Lmin4zshkJ', 0)\n",
      "(u'I liked a @YouTube video http://t.co/PmUVLjCG6g Ariana Grande - Problem ft. Iggy Azalea', 0)\n",
      "(u\"RT @RichOrb: Watch @ArianaGrande's new video for Problem (feat. @IGGYAZALEA) now! https://t.co/7xdgbqErgC\", 0)\n",
      "(u'https://t.co/P0oMurlBeQ #PROBLEMVIDEO', 0)\n",
      "(u'https://t.co/Gur9FatcMA', 0)\n",
      "(u'YAAAAAS https://t.co/Jqe65ILsPF GUDSJIDJUWSJA', 0)\n",
      "(u'RT @thatismarina: THE #PROBLEMVIDEO IS OUT. https://t.co/AnymqSUoa5', 0)\n",
      "(u'OMG ITS OUT GUYS &gt;&gt; https://t.co/wPW5nAg0LW', 0)\n",
      "Bottom 10 tweets:\n",
      "(u'@ArianaGrande Problem slays my life. Problem slays your life. Problem over everything. Problem slays. https://t.co/b0jyBCGpXn', -4)\n",
      "(u'Ariana Grande - Problem ft. Iggy Azalea: http://t.co/dD532P8tH0 via @YouTube', -1)\n",
      "(u'SAIU!!! Vem assistir o videoclipe de Problem! http://t.co/EyM3LNPWdk #ProblemVideoOnVevo', -1)\n",
      "(u'Ariana Grande - Problem ft. Iggy Azalea: http://t.co/DhFkLlk22E ya est\\xe1 \\u2665\\u2665\\u2665\\u2665', -1)\n",
      "(u'Me gust\\xf3 un video de @YouTube http://t.co/C08iGztdBN Ariana Grande - Problem ft. Iggy Azalea', -1)\n",
      "(u'Ariana Grande - Problem ft. Iggy Azalea: http://t.co/USaWjbYH3n', -1)\n",
      "(u'THE PROBLEM MUSIC VIDEO http://t.co/MvHRBRo7fw', -1)\n",
      "(u\"Ariana Grande - Problem ft. Iggy Azalea Oh my gosh it's finally here!! \\U0001f60d\\U0001f495\\U0001f44c @ArianaGrande  http://t.co/5JiHIkHtEJ\", -1)\n",
      "(u'OMG\\U0001f481\\U0001f497\\U0001f631\\U0001f631\\U0001f631\\U0001f631\\U0001f60d\\U0001f60d\\U0001f60d\\U0001f60d\\U0001f60d\\U0001f44c\\U0001f44c\\U0001f44c\\U0001f44c\\U0001f44c\\U0001f44c\\U0001f44c #PROBLEM @ArianaGrande https://t.co/oHrxsYzZbK', -1)\n"
     ]
    }
   ],
   "source": [
    "# code or answer for question. In case of plain text answer, change the type of cell to \"Raw Text\" using the toolbar.\n",
    "\n",
    "# import bz2, json\n",
    "reader = bz2.BZ2File(\"./twitter-dump.json.bz2\", mode=\"r\")\n",
    "jobj = json.loads(reader.readline())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Score={}\n",
    "for i in jobj[u'text']:\n",
    "    jobj = json.loads(reader.readline())\n",
    "    \n",
    "     \n",
    "    if jobj.has_key(u'text'):\n",
    "        tweet = jobj[u'text']\n",
    "        Score[tweet]=get_sentiment_score(jobj[u'text'])\n",
    "#print Score\n",
    "\n",
    "        \n",
    "t = sorted(Score.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "print \"Top 10 tweets:\"\n",
    "for i in range(0,9):\n",
    "    print t[i] \n",
    "\n",
    "        \n",
    "t = sorted(Score.iteritems(), key=lambda x:x[1])\n",
    "print \"Bottom 10 tweets:\"\n",
    "for i in range(0,9):\n",
    "    print t[i]    \n",
    "   \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 point) Assignment question #3.2:** Based on the scores calculated in **Assignment #3.1**, determine the 3 most positive users. A user $u_1$ is more positive than a user $u_2$ if the dataset contains more positive tweets emitted by $u_1$ than tweets emitted by $u_2$. Formally:\n",
    "$$ positivity(u_1) > positivity(u_2) \\iff \\left| \\left\\{ t \\, \\middle| \\, author(t) = u_1 \\wedge score(t) > 0 \\right\\} \\right| > \\left| \\left\\{ t \\, \\middle| \\, author(t) = u_2 \\wedge score(t) > 0 \\right\\} \\right|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 Positive Users\n",
      "(1243188937, 28)\n",
      "(1601537689, 25)\n",
      "(2229663685, 22)\n"
     ]
    }
   ],
   "source": [
    "# code or answer for question. In case of plain text answer, change the type of cell to \"Raw Text\" using the toolbar.\n",
    "reader = bz2.BZ2File(\"./twitter-dump.json.bz2\", mode=\"r\")\n",
    "Score={}\n",
    "for line in reader:\n",
    "    # load the JSON object from the read line\n",
    "    jobj = json.loads(line)\n",
    "    \n",
    "    # if the current tweet is a retweet\n",
    "    if jobj.has_key(u'text'):\n",
    "        # determine user id\n",
    "        user_id = jobj[u'user'][u'id']\n",
    "        if isinstance(user_id, dict):\n",
    "            user_id = user_id.values()[0]\n",
    "        user_id = int(user_id)\n",
    "        tweet = user_id\n",
    "        if (get_sentiment_score(jobj[u'text']) > 0):\n",
    "            if Score.has_key(tweet):\n",
    "                Score[tweet]+=1\n",
    "            else:\n",
    "                Score[tweet]=1\n",
    "\n",
    "#print Score\n",
    "\n",
    "t = sorted(Score.items(), key=lambda x: -x[1])\n",
    "print \"Top 3 Positive Users\"\n",
    "for i in range(0,3):\n",
    "    print t[i]   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 point) Assignment question #3.3:** \n",
    "We have discussed earlier that our system is fragile to negations: it will score the expression *not beautiful* as positive because it only detects the word beautiful as positive. \n",
    "More generally, we consider that the token **not** changes the polarity of a given token: **not beautiful** becomes negative, while **not bad** becomes positive.  \n",
    "Modify the function `get_sentiment_score(text)` to detect the changes of polarity due to the token **not**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: 'I love this cars', score: 1\n",
      "------------------------------------\n",
      "Tweet: 'This view is amazing', score: 1\n",
      "------------------------------------\n",
      "Tweet: 'I feel great this morning', score: 1\n",
      "------------------------------------\n",
      "Tweet: 'I am so excited about the concert', score: 1\n",
      "------------------------------------\n",
      "Tweet: 'He is my best friend', score: 1\n",
      "------------------------------------\n",
      "Tweet: 'I do not like this car', score: -1\n",
      "------------------------------------\n",
      "Tweet: 'This view is horrible', score: -1\n",
      "------------------------------------\n",
      "Tweet: 'I feel tired this morning', score: -1\n",
      "------------------------------------\n",
      "Tweet: 'I am not looking forward to the concert', score: -1\n",
      "------------------------------------\n",
      "Tweet: 'He is my worst enemy', score: -2\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# code or answer for question. In case of plain text answer, change the type of cell to \"Raw Text\" using the toolbar.\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# define the lemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "# read the positive and negative lexicon in lists of words\n",
    "positive_words = [lmtzr.lemmatize(line.strip().decode('utf-8')) for line in open('sentiment-lexicon-positive-words.txt')]\n",
    "negative_words = [lmtzr.lemmatize(line.strip().decode('utf-8')) for line in open('sentiment-lexicon-negative-words.txt')]\n",
    "\n",
    "# define the function that computes the sentiment score\n",
    "def get_sentiment_score(text):\n",
    "    # tokenize and lemmatize the current tweet\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tweet = [lmtzr.lemmatize(x.lower()) for x in tokens if len(x) >= 3]\n",
    "\n",
    "    # calculate the sentiment score\n",
    "    score = 0\n",
    "   # for word in tweet:\n",
    "    for i in range(0,len(tweet)):\n",
    "        if tweet[i] in positive_words:\n",
    "            if i>0:\n",
    "                if tweet[i-1] == \"not\":\n",
    "                    score = score - 1\n",
    "                else:\n",
    "                    score = score + 1 \n",
    "            else:\n",
    "                 score = score + 1\n",
    "        if tweet[i] in negative_words:\n",
    "            if i>0:\n",
    "                if tweet[i-1] == \"not\":\n",
    "                    score = score + 1\n",
    "                else:\n",
    "                    score = score - 1 \n",
    "            else:\n",
    "                 score = score - 1\n",
    "        if tweet[i] == \"not\":\n",
    "            if (tweet[i+1] in positive_words) or (tweet[i+1] in negative_words):\n",
    "                continue\n",
    "            else:\n",
    "                score = score -1\n",
    "    return score\n",
    "\n",
    "for words in pos_tweets + neg_tweets:\n",
    "    print \"Tweet: '{:s}', score: {:d}\".format(words, get_sentiment_score(words))\n",
    "    print \"------------------------------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus assignments questions\n",
    "---\n",
    "Bonus assignment questions earn you extra marks if you solve this assignment correctly, no penalty is inflicted if you do not solve it. \n",
    "Note that the total grade of SMA assignments cannot exceed 10 points, therefore the bonus point can only be used to compensate for another question which you did not solve correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.25 additional points) Bonus question #1.** \n",
    "Based on the scores calculated in **Assignment #3.1**, plot the temporal evolution of the counts of positive and negative tweets. The date a tweet was emitted is found in the field *created_at*. \n",
    "Divide the temporal extent of your dataset into 100 timeslices. \n",
    "The temporal extent of the dataset is from the creation date of the first tweet to the creation date of the last tweet. \n",
    "Count how many positive and how many negative tweets you have in each timeslice. \n",
    "Plot these counts on a graphic resembling this one:  \n",
    "![caption](files/desired-plot.png)  \n",
    "**HINT:** the graphic above is not based on real data. Your actual curves might **NOT** look like this one. Its purpose is just to show you the expected form of the graphic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code or answer for question. In case of plain text answer, change the type of cell to \"Raw Text\" using the toolbar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.25 additional points) Bonus question #2: Polarized communities** We want to know if there are polarized communities in our social graph. Are there closely linked groups of users who have similar sentiment polarities? We calculate the sentiment polarity of a user as the numbre of positive tweets emitted, from which we substract the number of negative tweets. Formally:\n",
    "$$sentiment\\_polarity(u) = \\left| \\left\\{ t \\, \\middle| \\, author(t) = u \\wedge score(t) > 0 \\right\\} \\right| - \\left| \\left\\{ t \\, \\middle| \\, author(t) = u \\wedge score(t) < 0 \\right\\} \\right| \\enspace.$$  \n",
    "To visually detect the polarized communities, we want to plot the same graph as in [tutorial 1](https://wattlecourses.anu.edu.au/pluginfile.php/1510161/mod_resource/content/13/tutorial-1-construct-social-graph.html) at *Step 3*, but with the colors of nodes representing their polarity: `blue` for a negative polarity and `red` for a positive polarity. \n",
    "Are nodes colored similarly clustered close together?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code or answer for question. In case of plain text answer, change the type of cell to \"Raw Text\" using the toolbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
